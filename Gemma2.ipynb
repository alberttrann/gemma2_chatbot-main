{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3MMAcssHTML"
      },
      "source": [
        "<link rel=\"stylesheet\" href=\"/site-assets/css/gemma.css\">\n",
        "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Google+Symbols:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "402c3d8a"
      },
      "source": [
        "# Xây một con chatbot với Gemma 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b686fd95"
      },
      "source": [
        "Large Language Models (LLMs) như Gemma thì làm tốt việc generating informative responses, nên mình sẽ xài nó để build trợ lý ảo với chatbot.\n",
        "\n",
        "Thường thì mấy LLM sẽ hoạt động trong một stateless manner, tức là nó không có inherent memory để lưu trữ đoạn hội thoại cũ. Mỗi prompt hay mỗi question sẽ được processed độc lập, không liên quan tới những tương tác trước đó. Nhưng mà một điều quan trọng của natural conversation là phải giữ được context trước đó. Để vượt qua hạn chế này và cho phép LLM maintain được conversation context (or pertinent parts), LLM phải dược cung cấp một cách rõ ràng các thông tin có liên quan chẳng hạn như lịch sử cuộc trò chuyện cho mỗi một prompt mới được present đến LLM.\n",
        "\n",
        "Đây là tutorial để build một con chatbot, xài instruction-tuned model variant của Gemma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29732090"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ6W7NzRe1VM"
      },
      "source": [
        "### Gemma setup\n",
        "\n",
        "* Tìm Gemma model card trên kaggle.com, request access và điền consent form để kaggle duyệt quyền xài model\n",
        "* Chọn Colab Runtime phù hợp, ở chỗ góc cao bên phải của Google Colab, chọn T4 GPU\n",
        "* Vào mục Public key của Kaggle để download file json có chứa username và API key để add vào trong mục secrets của Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gN-IVRC3dQe"
      },
      "source": [
        "### Set environment variables cho `KAGGLE_USERNAME` và `KAGGLE_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrBoa_Urw9Vx"
      },
      "outputs": [],
      "source": [
        "# Note: `userdata.get` là một Colab API. Nếu không dùng Colab thì set env\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"hngtrnvminh\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"9d6234a4e515964fd29db30c0d0367d1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9oy3QUmXtSd"
      },
      "source": [
        "### Tải Keras với KerasNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a973dd7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf76395-8cce-45fa-9eeb-ec628842f599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m230.2/230.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Tải Keras 3.  https://keras.io/getting_started/\n",
        "# tensorflow-cpu để đảm bảo tính tương thích với mấy thư viện tensorflow-specific, nhưng mà mặc định thì sẽ xài gpu nếu có, mà google colab thì có T4\n",
        "# keras-nlp nó có sẵn kha khá model và tool được prebuilt như là GemmaCausalLM, thì lúc triển khai nó dễ hơn\n",
        "# tensorflow hub với text để thêm tiện ích cho text processing với pretrained model\n",
        "!pip install -q tensorflow-cpu\n",
        "!pip install -q -U keras-nlp tensorflow-hub\n",
        "!pip install -q -U \"keras>=3\"\n",
        "!pip install -q -U tensorflow-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wme8666dUPVR"
      },
      "source": [
        "### Lựa backend\n",
        "\n",
        "Keras là một high-level, multi-framework deep learning API thiết kế hướng đến sự đơn giản và dễ xài và sẽ cho mình chọn một trong 3 backend: TensorFlow, JAX, or PyTorch. Cả 3 đều sẽ work được trong cái tutorial này hết"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67d12d2d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Chọn Jax làm backend\n",
        "# Jax cung cấp khả năng tính toán hiệu quả trên accelerator như là TPU với GPU kiểu công nghiệp, ở đây mình có GPU xịn của Google nên mình sẽ xài Jax\n",
        "# So với pytorch thì jax cũng cho tốc độ tốt hơn trong nhiều trường hợp, đặc biệt là trên TPU GPU chuyên dụng\n",
        "# Với các dự án dùng consumer-grade gpu như là mấy cái rtx trên laptop thì mình nên compile bằng torch tại tương thích sẽ tốt hơn\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "# Pre-allocate 100% TPU memory để giảm thiểu phân mảnh(memory fragmentation) với tăng perf\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajm_SGWTUjVd"
      },
      "source": [
        "### Import Keras, KerasNLP và set seed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lyn9FxPUok8"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "\n",
        "# reproducibility này là kiểu chỉ số để đảm bảo con chatbot nó không trả lời quá khác nhau giữa những lần hỏi khác nhau mà cùng nội dung á\n",
        "# cái số 42 nó hong có gì đặc biệt đâu kiểu là để model biết là sự ngẫu nhiên giữa những câu trả lời cho mỗi câu hỏi giống nhau sẽ không quá nhiều miễn là cái seed này được set ở 42\n",
        "keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39dc9d5b"
      },
      "source": [
        "### Khởi tạo model\n",
        "\n",
        "KerasNLP cung cấp rất nhiều implementation cho các [model architectures](https://keras.io/api/keras_nlp/models/) nổi bật. Trong notebook này thì mình sẽ khởi tạo model bằng `GemmaCausalLM`, một model Gemma end-to-end cho causal language modeling. Một causal language model thì dự đoán token tiếp theo dựa trên những token trước đó.\n",
        "\n",
        "Khởi tạo model bằng method `from_preset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c86dc8fe"
      },
      "outputs": [],
      "source": [
        "#Xài kerasnlp để load model, này là giải pháp cực kì optimized và được đóng gói sẵn hết\n",
        "#khỏi phải lo về mấy cái setup phức tạp như là AutoModelForCausalLM hay là tự build thủ công tokenizer với model, giảm bớt rất nhiều bottleneck\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcCv0BSdVFv9"
      },
      "source": [
        " `GemmaCausalLM.from_preset()` function khởi tạo model từ một preset architecture và weights. String `\"gemma2_instruct_2b_en\"` chỉ cụ thể preset the Gemma 2 2B model với 2 tỉ parameters. Model Gemma với [7B, 9B, and 27B parameters](https://ai.google.com/gemma/docs/get_started#models-list) cũng available. Có thể tìm thấy code string cho mấy model đó ở  **Model Variation** listings trên [Kaggle](https://www.kaggle.com/models/google/gemma).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLNx8AoeVe-a"
      },
      "source": [
        "Xài `summary` method để lấy thông tin về model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MorieIpVksu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "bbb25f48-2077-46af-816d-9352e2e2aec1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              │                      Vocab size: \u001b[38;5;34m256,000\u001b[0m │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                                                  </span>┃<span style=\"font-weight: bold\">                                   Config </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              │                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> │\n",
              "└───────────────────────────────────────────────────────────────┴──────────────────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        │   \u001b[38;5;34m2,614,341,888\u001b[0m │ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        │\n",
              "│ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               │                           │                 │ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      │     \u001b[38;5;34m589,824,000\u001b[0m │ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
              "│ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                  </span>┃<span style=\"font-weight: bold\"> Output Shape              </span>┃<span style=\"font-weight: bold\">         Param # </span>┃<span style=\"font-weight: bold\"> Connected to               </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                          │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ gemma_backbone                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        │   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> │ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               │                           │                 │ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────────┼───────────────────────────┼─────────────────┼────────────────────────────┤\n",
              "│ token_embedding               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      │     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> │ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         │                           │                 │                            │\n",
              "└───────────────────────────────┴───────────────────────────┴─────────────────┴────────────────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArZPOzFpVp6S"
      },
      "source": [
        "Từ summary thấy được model có 2.6 tỉ trainable parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WpS39TBYql9"
      },
      "source": [
        "### Define formatting helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-obTC1jZGpZ"
      },
      "outputs": [],
      "source": [
        "# Này là mình format chat output, kiểu nhìn nó như nào á, ở đây mình format bằng Markdown trong thư viện IPython của Jupyter\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "def display_chat(prompt, text):\n",
        "  formatted_prompt = \"<font size='+1' color='brown'>🙋‍♂️<blockquote>\" + prompt + \"</blockquote></font>\"\n",
        "  text = text.replace('•', '  *')\n",
        "  text = textwrap.indent(text, '> ', predicate=lambda _: True)\n",
        "  formatted_text = \"<font size='+1' color='teal'>🤖\\n\\n\" + text + \"\\n</font>\"\n",
        "  return Markdown(formatted_prompt+formatted_text)\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('•', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ca54e8c"
      },
      "source": [
        "## Build chatbot\n",
        "\n",
        "Gemma instruction-tuned model `gemma2_instruct_2b_en` được fine-tuned để hiểu được turn tokens như sau:\n",
        "\n",
        "```\n",
        "<start_of_turn>user\\n  ... <end_of_turn>\\n\n",
        "<start_of_turn>model\\n ... <end_of_turn>\\n\n",
        "```\n",
        "\n",
        "Tutorial cũng dùng những tokens đó để build chatbot. Xem thêm ở [Formatting and system instructions](https://ai.google.dev/gemma/docs/formatting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9583dfd1"
      },
      "source": [
        "### Tạo chat helper để quản lý conversation state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4e9a187"
      },
      "outputs": [],
      "source": [
        "# Đây là main logic để quản lý cuộc hội thoại của chatbot\n",
        "# System and History Management: Tracks messages bằng markers (<start_of_turn> và <end_of_turn>) cho cả user và model\n",
        "# Prompt Construction: Builds một conversation context hoàn chỉnh (get_full_prompt) bằng cách kết hợp cả history và system instructions\n",
        "# Gửi message: Thêm user input vào history -> build prompt -> generate ra response bằng generate method của model -> update history với phản hồi từ model\n",
        "class ChatState():\n",
        "\n",
        "  __START_TURN_USER__ = \"<start_of_turn>user\\n\"\n",
        "  __START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n",
        "  __END_TURN__ = \"<end_of_turn>\\n\"\n",
        "\n",
        "  def __init__(self, model, system=\"\"):\n",
        "\n",
        "    self.model = model\n",
        "    self.system = system\n",
        "    self.history = []\n",
        "\n",
        "  def add_to_history_as_user(self, message):\n",
        "\n",
        "      self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n",
        "\n",
        "  def add_to_history_as_model(self, message):\n",
        "\n",
        "      self.history.append(self.__START_TURN_MODEL__ + message)\n",
        "\n",
        "  def get_history(self):\n",
        "\n",
        "      return \"\".join([*self.history])\n",
        "\n",
        "  def get_full_prompt(self):\n",
        "\n",
        "    prompt = self.get_history() + self.__START_TURN_MODEL__\n",
        "    if len(self.system)>0:\n",
        "      prompt = self.system + \"\\n\" + prompt\n",
        "    return prompt\n",
        "\n",
        "  def send_message(self, message):\n",
        "\n",
        "    self.add_to_history_as_user(message)\n",
        "    prompt = self.get_full_prompt()\n",
        "    response = self.model.generate(prompt, max_length=2048)\n",
        "    result = response.replace(prompt, \"\")\n",
        "    self.add_to_history_as_model(result)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hmJS4h4ZmiP"
      },
      "source": [
        "### Chat với model\n",
        "\n",
        "Bắt đầu chat với model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1913181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "outputId": "e048bcae-0753-4505-ce3d-9534153f63b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Explain Riemann hypothesis to me like I'm a high schoooler</blockquote></font><font size='+1' color='teal'>🤖\n\n> Imagine you have a giant, endless sheet of paper.  You want to fill it with numbers, but there's a catch: you can only use whole numbers.  \n> \n> Now, imagine you're trying to find all the \"prime numbers\" on this sheet. Prime numbers are like the special, unique building blocks of numbers. They can only be divided by 1 and themselves.  \n> \n> The Riemann Hypothesis is about understanding the distribution of these prime numbers on this paper. It's a big, complex mathematical puzzle that scientists have been trying to solve for over a century.\n> \n> Here's the gist:\n> \n> * **Prime Numbers:**  Prime numbers are like the special, unique building blocks of numbers. They can only be divided by 1 and themselves.\n> * **The Riemann Zeta Function:** This is a special mathematical function that helps us understand the distribution of prime numbers. It's like a map that shows us where prime numbers are likely to appear.\n> * **The Riemann Hypothesis:** This hypothesis states that the distribution of prime numbers follows a very specific pattern, and this pattern can be described by the Riemann Zeta Function.\n> \n> **Why is it so important?**\n> \n> * **Cryptography:** Prime numbers are the foundation of modern encryption, which keeps our online information safe. Understanding the Riemann Hypothesis could help us develop even stronger encryption methods.\n> * **Number Theory:** The Riemann Hypothesis is a fundamental problem in number theory, the study of numbers and their properties. Solving it would revolutionize our understanding of numbers.\n> * **Mathematics in general:**  It's a big deal because it connects different areas of mathematics, like algebra, geometry, and analysis.\n> \n> **The Challenge:**\n> \n> The Riemann Hypothesis is incredibly difficult to prove. It's like trying to solve a puzzle with millions of pieces, and even the most brilliant mathematicians have struggled to crack it.\n> \n> **In a nutshell:**\n> \n> The Riemann Hypothesis is a big, unsolved puzzle about prime numbers and their distribution. It's a fascinating problem that could have major implications for cryptography, mathematics, and our understanding of the universe. \n> \n> \n> Let me know if you'd like me to explain any of these concepts in more detail! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Khởi tạo chatbot và gửi query đến chatbot\n",
        "chat = ChatState(gemma_lm)\n",
        "message = \"Explain Riemann hypothesis to me like I'm a high schoooler\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7448005b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96eb2e43-0ae4-4a66-bc26-86e16a9a6cda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Gimme code to compute all prime numbers up to 100</blockquote></font><font size='+1' color='teal'>🤖\n\n> ```python\n> def is_prime(num):\n>   \"\"\"Checks if a number is prime.\"\"\"\n>   if num <= 1:\n>     return False\n>   if num <= 3:\n>     return True\n>   if num % 2 == 0 or num % 3 == 0:\n>     return False\n>   i = 5\n>   while i * i <= num:\n>     if num % i == 0 or num % (i + 2) == 0:\n>       return False\n>     i += 6\n>   return True\n> \n> primes = []\n> for num in range(2, 101):\n>   if is_prime(num):\n>     primes.append(num)\n> \n> print(\"Prime numbers up to 100:\", primes)\n> ```\n> \n> **Explanation:**\n> \n> 1. **`is_prime(num)` function:**\n>    - Takes an integer `num` as input.\n>    - Returns `True` if `num` is prime, `False` otherwise.\n>    - Optimizations:\n>      - Handles numbers less than or equal to 3 directly.\n>      - Checks divisibility by 2 and 3 first.\n>      - Uses the 6k ± 1 optimization: All prime numbers greater than 3 can be expressed in the form 6k ± 1 (where k is any integer).\n> \n> 2. **Main loop:**\n>    - Iterates through numbers from 2 to 100.\n>    - For each number, calls the `is_prime()` function.\n>    - If the function returns `True`, the number is added to the `primes` list.\n> \n> 3. **Output:**\n>    - Prints the list of prime numbers found.\n> \n> **How it works:**\n> \n> The code efficiently checks for primality by:\n> \n> - **Early optimization:**  It handles special cases for numbers less than or equal to 3.\n> - **6k ± 1 optimization:**  It checks divisibility only by numbers of the form 6k ± 1, significantly reducing the number of checks.\n> \n> \n> Let me know if you'd like to explore other prime number algorithms or have any more questions! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "message = \"Gimme code to compute all prime numbers up to 100\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0c51f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "outputId": "fe2d0cbf-9730-4deb-ff2e-0ab74baa25f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Great! Now add those explanations as comments in the code.</blockquote></font><font size='+1' color='teal'>🤖\n\n> ```python\n> def is_prime(num):\n>   \"\"\"Checks if a number is prime.\n> \n>   Args:\n>     num: The number to check.\n> \n>   Returns:\n>     True if num is prime, False otherwise.\n>   \"\"\"\n>   if num <= 1:  # Numbers less than or equal to 1 are not prime\n>     return False\n>   if num <= 3:  # 2 and 3 are prime\n>     return True\n>   if num % 2 == 0 or num % 3 == 0:  # Check divisibility by 2 and 3\n>     return False\n>   i = 5  # Start checking divisibility from 5\n>   while i * i <= num:  # Check divisibility up to the square root of num\n>     if num % i == 0 or num % (i + 2) == 0:  # If divisible by i or i+2, not prime\n>       return False\n>     i += 6  # Increment by 6 to check the next potential divisors\n>   return True  # If no divisors found, it's prime\n> \n> primes = []  # Initialize an empty list to store prime numbers\n> for num in range(2, 101):  # Iterate through numbers from 2 to 100\n>   if is_prime(num):  # Check if the number is prime\n>     primes.append(num)  # If prime, add it to the list\n> \n> print(\"Prime numbers up to 100:\", primes)  # Print the list of prime numbers\n> ``` \n> \n> \n> Let me know if you have any other questions! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "message = \"Great! Now add those explanations as comments in the code.\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a33627"
      },
      "source": [
        "Test thử code mà model đưa luôn:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "221c0817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9895d7ff-9629-4687-bb09-bcbef0f45e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prime numbers up to 100: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
          ]
        }
      ],
      "source": [
        "def is_prime(num):\n",
        "  \"\"\"Checks if a number is prime.\n",
        "\n",
        "  Args:\n",
        "    num: The number to check.\n",
        "\n",
        "  Returns:\n",
        "    True if num is prime, False otherwise.\n",
        "  \"\"\"\n",
        "  if num <= 1:  # Numbers less than or equal to 1 are not prime\n",
        "    return False\n",
        "  if num <= 3:  # 2 and 3 are prime\n",
        "    return True\n",
        "  if num % 2 == 0 or num % 3 == 0:  # Check divisibility by 2 and 3\n",
        "    return False\n",
        "  i = 5  # Start checking divisibility from 5\n",
        "  while i * i <= num:  # Check divisibility up to the square root of num\n",
        "    if num % i == 0 or num % (i + 2) == 0:  # If divisible by i or i+2, not prime\n",
        "      return False\n",
        "    i += 6  # Increment by 6 to check the next potential divisors\n",
        "  return True  # If no divisors found, it's prime\n",
        "\n",
        "primes = []  # Initialize an empty list to store prime numbers\n",
        "for num in range(2, 101):  # Iterate through numbers from 2 to 100\n",
        "  if is_prime(num):  # Check if the number is prime\n",
        "    primes.append(num)  # If prime, add it to the list\n",
        "\n",
        "print(\"Prime numbers up to 100:\", primes)  # Print the list of prime numbers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from IPython.display import display  # Ensures Markdown rendering\n",
        "\n",
        "def timed_chat(message):\n",
        "    start_time = time.time()\n",
        "    response = chat.send_message(message)\n",
        "    total_time = time.time() - start_time\n",
        "    tokens_generated = len(response.split())  # Approximate token count\n",
        "    tokens_per_sec = tokens_generated / total_time if total_time > 0 else 0\n",
        "\n",
        "    # ✅ Ensure chat response is displayed properly\n",
        "    display(display_chat(message, response))\n",
        "\n",
        "    # ✅ Print generation stats\n",
        "    print(f\"\\n⏱️ Total Generation Time: {total_time:.3f} sec | ⚡ Tokens/sec: {tokens_per_sec:.2f}\")\n"
      ],
      "metadata": {
        "id": "Sk3vkYm2VC_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timed_chat(\"Explain Riemann hypothesis to me like I'm a high schooler\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "IxOZ214SVFo5",
        "outputId": "2b07b6ad-8c29-4b48-e775-b383bc08c990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>🙋‍♂️<blockquote>Explain Riemann hypothesis to me like I'm a high schooler</blockquote></font><font size='+1' color='teal'>🤖\n\n> Okay, imagine a giant, endless sheet of paper.  You want to fill it with numbers, but there's a catch: you can only use whole numbers.  \n> \n> Now, imagine you're trying to find all the \"prime numbers\" on this sheet. Prime numbers are like the special, unique building blocks of numbers. They can only be divided by 1 and themselves. Think of 2, 3, 5, 7, 11... those are prime numbers.\n> \n> The Riemann Hypothesis is about understanding how these prime numbers are spread out on this paper. It's a big, complex mathematical puzzle that scientists have been trying to solve for over a century.\n> \n> Here's the gist:\n> \n> * **Prime Numbers:** Prime numbers are like the special, unique building blocks of numbers. They can only be divided by 1 and themselves.\n> * **The Riemann Zeta Function:** This is a special mathematical function that helps us understand the distribution of prime numbers. It's like a map that shows us where prime numbers are likely to appear.\n> * **The Riemann Hypothesis:** This hypothesis states that the distribution of prime numbers follows a very specific pattern, and this pattern can be described by the Riemann Zeta Function.\n> \n> **Why is it so important?**\n> \n> * **Cryptography:** Prime numbers are the foundation of modern encryption, which keeps our online information safe. Understanding the Riemann Hypothesis could help us develop even stronger encryption methods.\n> * **Number Theory:** The Riemann Hypothesis is a fundamental problem in number theory, the study of numbers and their properties. Solving it would revolutionize our understanding of numbers.\n> * **Mathematics in general:**  It's a big deal because it connects different areas of mathematics, like algebra, geometry, and analysis.\n> \n> **The Challenge:**\n> \n> The Riemann Hypothesis is incredibly difficult to prove. It's like trying to solve a puzzle with millions of pieces, and even the most brilliant mathematicians have struggled to crack it.\n> \n> **In a nutshell:**\n> \n> The Riemann Hypothesis is a big, unsolved puzzle about prime numbers and their distribution. It's a fascinating problem that could have major implications for cryptography, mathematics, and our understanding of the universe. \n> \n> \n> Let me know if you'd like me to explain any of these concepts in more detail! \n> <end_of_turn>\n</font>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "⏱️ Total Generation Time: 29.055 sec | ⚡ Tokens/sec: 12.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c8ece6c"
      },
      "source": [
        "Xài `get_history` method để xem context được retained bởi `Chat` class như nào"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e48f4ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591aa747-47f3-4871-a1c1-a71363708d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start_of_turn>user\n",
            "Tell me, in a few words,  how to compute all prime numbers up to 1000?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "**Sieve of Eratosthenes.** \n",
            "<end_of_turn><start_of_turn>user\n",
            "Now in Python! No numpy, please!<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Returns a list of prime numbers up to n.\"\"\"\n",
            "  primes = [True] * (n + 1)\n",
            "  primes[0] = primes[1] = False\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    if primes[i]:\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        primes[j] = False\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1. **Initialization:**\n",
            "   - `primes = [True] * (n + 1)`: Creates a list `primes` of boolean values, initially assuming all numbers are prime.\n",
            "   - `primes[0] = primes[1] = False`: Sets 0 and 1 as non-prime.\n",
            "\n",
            "2. **Iteration:**\n",
            "   - `for i in range(2, int(n**0.5) + 1):`: Iterates from 2 to the square root of `n`. We only need to check up to the square root because any composite number must have a prime factor less than or equal to its square root.\n",
            "   - `if primes[i]:`: If `i` is marked as prime:\n",
            "     - `for j in range(i * i, n + 1, i):`: Marks all multiples of `i` as non-prime.\n",
            "\n",
            "3. **Result:**\n",
            "   - `return [i for i, is_prime in enumerate(primes) if is_prime]`: Creates a list of indices where `primes[i]` is True, representing the prime numbers.\n",
            "\n",
            "\n",
            "Let me know if you'd like a more detailed explanation of any part! \n",
            "<end_of_turn><start_of_turn>user\n",
            "Thank you, it works! Can you explain the code in French?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Bien sûr ! Voici une explication du code en français :\n",
            "\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Retourne une liste de nombres premiers jusqu'à n.\"\"\"\n",
            "  primes = [True] * (n + 1)\n",
            "  primes[0] = primes[1] = False\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    if primes[i]:\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        primes[j] = False\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explication:**\n",
            "\n",
            "1. **Initialisation:**\n",
            "   - `primes = [True] * (n + 1)`: Crée une liste `primes` de valeurs booléennes, initialement supposant que tous les nombres sont premiers.\n",
            "   - `primes[0] = primes[1] = False`: Définit 0 et 1 comme non-premiers.\n",
            "\n",
            "2. **Itération:**\n",
            "   - `for i in range(2, int(n**0.5) + 1):`: Itère de 2 jusqu'à la racine carrée de `n`. Nous ne devons vérifier que jusqu'à la racine carrée car tout nombre composite doit avoir un facteur premier inférieur ou égal à sa racine carrée.\n",
            "   - `if primes[i]:`: Si `i` est considéré comme premier:\n",
            "     - `for j in range(i * i, n + 1, i):`:  Marquer tous les multiples de `i` comme non-premiers.\n",
            "\n",
            "3. **Resultat:**\n",
            "   - `return [i for i, is_prime in enumerate(primes) if is_prime]`: Crée une liste des indices où `primes[i]` est vrai, représentant les nombres premiers.\n",
            "\n",
            "\n",
            "N'hésitez pas à me demander si vous avez besoin d'une explication plus détaillée de quelque chose! \n",
            "<end_of_turn><start_of_turn>user\n",
            "Great! Now add those explanations as comments in the code.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Retourne une liste de nombres premiers jusqu'à n.\"\"\"\n",
            "  # Initialise une liste de boolean avec True pour tous les nombres de 0 à n\n",
            "  primes = [True] * (n + 1)\n",
            "  # Définit 0 et 1 comme non-premiers\n",
            "  primes[0] = primes[1] = False\n",
            "  # Itère de 2 à la racine carrée de n\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    # Si i est considéré comme premier\n",
            "    if primes[i]:\n",
            "      # Itère sur tous les multiples de i\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        # Définit les multiples de i comme non-premiers\n",
            "        primes[j] = False\n",
            "  # Retourne la liste des indices des nombres premiers\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explication:**\n",
            "\n",
            "* **Initialisation:**\n",
            "    * `primes = [True] * (n + 1)`:  Crée une liste `primes` de valeurs booléennes, initialement supposant que tous les nombres sont premiers.\n",
            "    * `primes[0] = primes[1] = False`: Définit 0 et 1 comme non-premiers.\n",
            "* **Itération:**\n",
            "    * `for i in range(2, int(n**0.5) + 1):`: Itère de 2 jusqu'à la racine carrée de `n`. Nous ne devons vérifier que jusqu'à la racine carrée car tout nombre composite doit avoir un facteur premier inférieur ou égal à sa racine carrée.\n",
            "    * `if primes[i]:`: Si `i` est considéré comme premier:\n",
            "        * `for j in range(i * i, n + 1, i):`:  Marquer tous les multiples de `i` comme non-premiers.\n",
            "* **Resultat:**\n",
            "    * `return [i for i, is_prime in enumerate(primes) if is_prime]`: Crée une liste des indices où `primes[i]` est vrai, représentant les nombres premiers. \n",
            "\n",
            "\n",
            "\n",
            "<end_of_turn>\n"
          ]
        }
      ],
      "source": [
        "print(chat.get_history())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9693c66f"
      },
      "source": [
        "## Tổng kết và đọc thêm\n",
        "\n",
        "Vậy là xong phần build chatbot với Gemma 2B Instruction tuned model xài Keras on JAX.\n",
        "\n",
        "Đọc thêm:\n",
        "\n",
        "* [Get started with Keras Gemma](https://ai.google.dev/gemma/docs/get_started).\n",
        "* [Finetune the Gemma model on GPU](https://ai.google.dev/gemma/docs/lora_tuning).\n",
        "* [Gemma integration with Vertex AI](https://ai.google.dev/gemma/docs/integrations/vertex)\n",
        "* [Gemma on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ưu điểm của setup sử dụng Google Colab\n",
        "* *Ease of setup*: Code và dependencies minimal hơn, trong khi Local thì cần nhiều setup hơn cho tokenizer, model và server. keras_nlp.models.GemmaCausalLM.from_preset nó load trực tiếp nguyên một cái pre-trained model đã được tối ưu, không cần phải lo thêm về handling, memory optimization hay set up tokenizer\n",
        "* *Harware Uitilization*: Tương thích với T4 GPU(qua Jax), Local thì cần cấu hình thủ công cho CPU và GPU\n",
        "* *Model loading*: Pre-trained thông qua preset của Keras-NLP, Local thì cần loading thủ công thông qua transformers, cần nhiều VRAM hơn\n",
        "* *Framework*: Jax(nhanh hơn và tối ưu trên TPU GPU chuyên dụng)\n",
        "* *UI*: Colab(Markdown trong Colab, module IPython của Jupyter gọn nhẹ)\n",
        "* *Tốc độ Inference*: Nhờ Jax nên rất tối ưu là một, Keras-NLP lại còn lo hết từ batching tới memory tới token generation nữa nên siêu tối ưu, còn giải pháp Local thì cần tự làm batching, cũng kha khá phức tạp và tăng batch size lên thì cũng cần nhiều vram, với mình thử tự set batch_size thì lỗi nhảy đỏ lè nên hong làm:>\n",
        "* *Backend*: Colab không có server overhead luôn mà là direct execution, mọi thứ nó chạy trong hết cùng 1 process."
      ],
      "metadata": {
        "id": "pDYv8pETF-qh"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}