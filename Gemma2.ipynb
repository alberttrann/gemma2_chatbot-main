{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3MMAcssHTML"
      },
      "source": [
        "<link rel=\"stylesheet\" href=\"/site-assets/css/gemma.css\">\n",
        "<link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/css2?family=Google+Symbols:opsz,wght,FILL,GRAD@20..48,100..700,0..1,-50..200\" />"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "402c3d8a"
      },
      "source": [
        "# XÃ¢y má»™t con chatbot vá»›i Gemma 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b686fd95"
      },
      "source": [
        "Large Language Models (LLMs) nhÆ° Gemma thÃ¬ lÃ m tá»‘t viá»‡c generating informative responses, nÃªn mÃ¬nh sáº½ xÃ i nÃ³ Ä‘á»ƒ build trá»£ lÃ½ áº£o vá»›i chatbot.\n",
        "\n",
        "ThÆ°á»ng thÃ¬ máº¥y LLM sáº½ hoáº¡t Ä‘á»™ng trong má»™t stateless manner, tá»©c lÃ  nÃ³ khÃ´ng cÃ³ inherent memory Ä‘á»ƒ lÆ°u trá»¯ Ä‘oáº¡n há»™i thoáº¡i cÅ©. Má»—i prompt hay má»—i question sáº½ Ä‘Æ°á»£c processed Ä‘á»™c láº­p, khÃ´ng liÃªn quan tá»›i nhá»¯ng tÆ°Æ¡ng tÃ¡c trÆ°á»›c Ä‘Ã³. NhÆ°ng mÃ  má»™t Ä‘iá»u quan trá»ng cá»§a natural conversation lÃ  pháº£i giá»¯ Ä‘Æ°á»£c context trÆ°á»›c Ä‘Ã³. Äá»ƒ vÆ°á»£t qua háº¡n cháº¿ nÃ y vÃ  cho phÃ©p LLM maintain Ä‘Æ°á»£c conversation context (or pertinent parts), LLM pháº£i dÆ°á»£c cung cáº¥p má»™t cÃ¡ch rÃµ rÃ ng cÃ¡c thÃ´ng tin cÃ³ liÃªn quan cháº³ng háº¡n nhÆ° lá»‹ch sá»­ cuá»™c trÃ² chuyá»‡n cho má»—i má»™t prompt má»›i Ä‘Æ°á»£c present Ä‘áº¿n LLM.\n",
        "\n",
        "ÄÃ¢y lÃ  tutorial Ä‘á»ƒ build má»™t con chatbot, xÃ i instruction-tuned model variant cá»§a Gemma."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29732090"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQ6W7NzRe1VM"
      },
      "source": [
        "### Gemma setup\n",
        "\n",
        "* TÃ¬m Gemma model card trÃªn kaggle.com, request access vÃ  Ä‘iá»n consent form Ä‘á»ƒ kaggle duyá»‡t quyá»n xÃ i model\n",
        "* Chá»n Colab Runtime phÃ¹ há»£p, á»Ÿ chá»— gÃ³c cao bÃªn pháº£i cá»§a Google Colab, chá»n T4 GPU\n",
        "* VÃ o má»¥c Public key cá»§a Kaggle Ä‘á»ƒ download file json cÃ³ chá»©a username vÃ  API key Ä‘á»ƒ add vÃ o trong má»¥c secrets cá»§a Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_gN-IVRC3dQe"
      },
      "source": [
        "### Set environment variables cho `KAGGLE_USERNAME` vÃ  `KAGGLE_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrBoa_Urw9Vx"
      },
      "outputs": [],
      "source": [
        "# Note: `userdata.get` lÃ  má»™t Colab API. Náº¿u khÃ´ng dÃ¹ng Colab thÃ¬ set env\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = \"hngtrnvminh\"\n",
        "os.environ[\"KAGGLE_KEY\"] = \"9d6234a4e515964fd29db30c0d0367d1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9oy3QUmXtSd"
      },
      "source": [
        "### Táº£i Keras vá»›i KerasNLP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a973dd7a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cf76395-8cce-45fa-9eeb-ec628842f599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m230.2/230.2 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Táº£i Keras 3.  https://keras.io/getting_started/\n",
        "# tensorflow-cpu Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh tÆ°Æ¡ng thÃ­ch vá»›i máº¥y thÆ° viá»‡n tensorflow-specific, nhÆ°ng mÃ  máº·c Ä‘á»‹nh thÃ¬ sáº½ xÃ i gpu náº¿u cÃ³, mÃ  google colab thÃ¬ cÃ³ T4\n",
        "# keras-nlp nÃ³ cÃ³ sáºµn kha khÃ¡ model vÃ  tool Ä‘Æ°á»£c prebuilt nhÆ° lÃ  GemmaCausalLM, thÃ¬ lÃºc triá»ƒn khai nÃ³ dá»… hÆ¡n\n",
        "# tensorflow hub vá»›i text Ä‘á»ƒ thÃªm tiá»‡n Ã­ch cho text processing vá»›i pretrained model\n",
        "!pip install -q tensorflow-cpu\n",
        "!pip install -q -U keras-nlp tensorflow-hub\n",
        "!pip install -q -U \"keras>=3\"\n",
        "!pip install -q -U tensorflow-text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wme8666dUPVR"
      },
      "source": [
        "### Lá»±a backend\n",
        "\n",
        "Keras lÃ  má»™t high-level, multi-framework deep learning API thiáº¿t káº¿ hÆ°á»›ng Ä‘áº¿n sá»± Ä‘Æ¡n giáº£n vÃ  dá»… xÃ i vÃ  sáº½ cho mÃ¬nh chá»n má»™t trong 3 backend: TensorFlow, JAX, or PyTorch. Cáº£ 3 Ä‘á»u sáº½ work Ä‘Æ°á»£c trong cÃ¡i tutorial nÃ y háº¿t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "67d12d2d"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Chá»n Jax lÃ m backend\n",
        "# Jax cung cáº¥p kháº£ nÄƒng tÃ­nh toÃ¡n hiá»‡u quáº£ trÃªn accelerator nhÆ° lÃ  TPU vá»›i GPU kiá»ƒu cÃ´ng nghiá»‡p, á»Ÿ Ä‘Ã¢y mÃ¬nh cÃ³ GPU xá»‹n cá»§a Google nÃªn mÃ¬nh sáº½ xÃ i Jax\n",
        "# So vá»›i pytorch thÃ¬ jax cÅ©ng cho tá»‘c Ä‘á»™ tá»‘t hÆ¡n trong nhiá»u trÆ°á»ng há»£p, Ä‘áº·c biá»‡t lÃ  trÃªn TPU GPU chuyÃªn dá»¥ng\n",
        "# Vá»›i cÃ¡c dá»± Ã¡n dÃ¹ng consumer-grade gpu nhÆ° lÃ  máº¥y cÃ¡i rtx trÃªn laptop thÃ¬ mÃ¬nh nÃªn compile báº±ng torch táº¡i tÆ°Æ¡ng thÃ­ch sáº½ tá»‘t hÆ¡n\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "# Pre-allocate 100% TPU memory Ä‘á»ƒ giáº£m thiá»ƒu phÃ¢n máº£nh(memory fragmentation) vá»›i tÄƒng perf\n",
        "os.environ[\"XLA_PYTHON_CLIENT_MEM_FRACTION\"] = \"1.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ajm_SGWTUjVd"
      },
      "source": [
        "### Import Keras, KerasNLP vÃ  set seed\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3lyn9FxPUok8"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import keras_nlp\n",
        "\n",
        "# reproducibility nÃ y lÃ  kiá»ƒu chá»‰ sá»‘ Ä‘á»ƒ Ä‘áº£m báº£o con chatbot nÃ³ khÃ´ng tráº£ lá»i quÃ¡ khÃ¡c nhau giá»¯a nhá»¯ng láº§n há»i khÃ¡c nhau mÃ  cÃ¹ng ná»™i dung Ã¡\n",
        "# cÃ¡i sá»‘ 42 nÃ³ hong cÃ³ gÃ¬ Ä‘áº·c biá»‡t Ä‘Ã¢u kiá»ƒu lÃ  Ä‘á»ƒ model biáº¿t lÃ  sá»± ngáº«u nhiÃªn giá»¯a nhá»¯ng cÃ¢u tráº£ lá»i cho má»—i cÃ¢u há»i giá»‘ng nhau sáº½ khÃ´ng quÃ¡ nhiá»u miá»…n lÃ  cÃ¡i seed nÃ y Ä‘Æ°á»£c set á»Ÿ 42\n",
        "keras.utils.set_random_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "39dc9d5b"
      },
      "source": [
        "### Khá»Ÿi táº¡o model\n",
        "\n",
        "KerasNLP cung cáº¥p ráº¥t nhiá»u implementation cho cÃ¡c [model architectures](https://keras.io/api/keras_nlp/models/) ná»•i báº­t. Trong notebook nÃ y thÃ¬ mÃ¬nh sáº½ khá»Ÿi táº¡o model báº±ng `GemmaCausalLM`, má»™t model Gemma end-to-end cho causal language modeling. Má»™t causal language model thÃ¬ dá»± Ä‘oÃ¡n token tiáº¿p theo dá»±a trÃªn nhá»¯ng token trÆ°á»›c Ä‘Ã³.\n",
        "\n",
        "Khá»Ÿi táº¡o model báº±ng method `from_preset`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c86dc8fe"
      },
      "outputs": [],
      "source": [
        "#XÃ i kerasnlp Ä‘á»ƒ load model, nÃ y lÃ  giáº£i phÃ¡p cá»±c kÃ¬ optimized vÃ  Ä‘Æ°á»£c Ä‘Ã³ng gÃ³i sáºµn háº¿t\n",
        "#khá»i pháº£i lo vá» máº¥y cÃ¡i setup phá»©c táº¡p nhÆ° lÃ  AutoModelForCausalLM hay lÃ  tá»± build thá»§ cÃ´ng tokenizer vá»›i model, giáº£m bá»›t ráº¥t nhiá»u bottleneck\n",
        "gemma_lm = keras_nlp.models.GemmaCausalLM.from_preset(\"gemma2_instruct_2b_en\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcCv0BSdVFv9"
      },
      "source": [
        " `GemmaCausalLM.from_preset()` function khá»Ÿi táº¡o model tá»« má»™t preset architecture vÃ  weights. String `\"gemma2_instruct_2b_en\"` chá»‰ cá»¥ thá»ƒ preset the Gemma 2 2B model vá»›i 2 tá»‰ parameters. Model Gemma vá»›i [7B, 9B, and 27B parameters](https://ai.google.com/gemma/docs/get_started#models-list) cÅ©ng available. CÃ³ thá»ƒ tÃ¬m tháº¥y code string cho máº¥y model Ä‘Ã³ á»Ÿ  **Model Variation** listings trÃªn [Kaggle](https://www.kaggle.com/models/google/gemma).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLNx8AoeVe-a"
      },
      "source": [
        "XÃ i `summary` method Ä‘á»ƒ láº¥y thÃ´ng tin vá» model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3MorieIpVksu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "outputId": "bbb25f48-2077-46af-816d-9352e2e2aec1"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mPreprocessor: \"gemma_causal_lm_preprocessor\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Preprocessor: \"gemma_causal_lm_preprocessor\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                                                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m                                  Config\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ gemma_tokenizer (\u001b[38;5;33mGemmaTokenizer\u001b[0m)                              â”‚                      Vocab size: \u001b[38;5;34m256,000\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                                                  </span>â”ƒ<span style=\"font-weight: bold\">                                   Config </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ gemma_tokenizer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaTokenizer</span>)                              â”‚                      Vocab size: <span style=\"color: #00af00; text-decoration-color: #00af00\">256,000</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"gemma_causal_lm\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"gemma_causal_lm\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                 \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape             \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mConnected to              \u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ padding_mask (\u001b[38;5;33mInputLayer\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_ids (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)              â”‚               \u001b[38;5;34m0\u001b[0m â”‚ -                          â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gemma_backbone                â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2304\u001b[0m)        â”‚   \u001b[38;5;34m2,614,341,888\u001b[0m â”‚ padding_mask[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],        â”‚\n",
              "â”‚ (\u001b[38;5;33mGemmaBackbone\u001b[0m)               â”‚                           â”‚                 â”‚ token_ids[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_embedding               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256000\u001b[0m)      â”‚     \u001b[38;5;34m589,824,000\u001b[0m â”‚ gemma_backbone[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       â”‚\n",
              "â”‚ (\u001b[38;5;33mReversibleEmbedding\u001b[0m)         â”‚                           â”‚                 â”‚                            â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                  </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape              </span>â”ƒ<span style=\"font-weight: bold\">         Param # </span>â”ƒ<span style=\"font-weight: bold\"> Connected to               </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ padding_mask (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_ids (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)              â”‚               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚ -                          â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ gemma_backbone                â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2304</span>)        â”‚   <span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> â”‚ padding_mask[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],        â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GemmaBackbone</span>)               â”‚                           â”‚                 â”‚ token_ids[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ token_embedding               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256000</span>)      â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">589,824,000</span> â”‚ gemma_backbone[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReversibleEmbedding</span>)         â”‚                           â”‚                 â”‚                            â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,614,341,888\u001b[0m (9.74 GB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,614,341,888</span> (9.74 GB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "gemma_lm.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArZPOzFpVp6S"
      },
      "source": [
        "Tá»« summary tháº¥y Ä‘Æ°á»£c model cÃ³ 2.6 tá»‰ trainable parameters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WpS39TBYql9"
      },
      "source": [
        "### Define formatting helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-obTC1jZGpZ"
      },
      "outputs": [],
      "source": [
        "# NÃ y lÃ  mÃ¬nh format chat output, kiá»ƒu nhÃ¬n nÃ³ nhÆ° nÃ o Ã¡, á»Ÿ Ä‘Ã¢y mÃ¬nh format báº±ng Markdown trong thÆ° viá»‡n IPython cá»§a Jupyter\n",
        "from IPython.display import Markdown\n",
        "import textwrap\n",
        "\n",
        "def display_chat(prompt, text):\n",
        "  formatted_prompt = \"<font size='+1' color='brown'>ğŸ™‹â€â™‚ï¸<blockquote>\" + prompt + \"</blockquote></font>\"\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  text = textwrap.indent(text, '> ', predicate=lambda _: True)\n",
        "  formatted_text = \"<font size='+1' color='teal'>ğŸ¤–\\n\\n\" + text + \"\\n</font>\"\n",
        "  return Markdown(formatted_prompt+formatted_text)\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ca54e8c"
      },
      "source": [
        "## Build chatbot\n",
        "\n",
        "Gemma instruction-tuned model `gemma2_instruct_2b_en` Ä‘Æ°á»£c fine-tuned Ä‘á»ƒ hiá»ƒu Ä‘Æ°á»£c turn tokens nhÆ° sau:\n",
        "\n",
        "```\n",
        "<start_of_turn>user\\n  ... <end_of_turn>\\n\n",
        "<start_of_turn>model\\n ... <end_of_turn>\\n\n",
        "```\n",
        "\n",
        "Tutorial cÅ©ng dÃ¹ng nhá»¯ng tokens Ä‘Ã³ Ä‘á»ƒ build chatbot. Xem thÃªm á»Ÿ [Formatting and system instructions](https://ai.google.dev/gemma/docs/formatting)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9583dfd1"
      },
      "source": [
        "### Táº¡o chat helper Ä‘á»ƒ quáº£n lÃ½ conversation state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e4e9a187"
      },
      "outputs": [],
      "source": [
        "# ÄÃ¢y lÃ  main logic Ä‘á»ƒ quáº£n lÃ½ cuá»™c há»™i thoáº¡i cá»§a chatbot\n",
        "# System and History Management: Tracks messages báº±ng markers (<start_of_turn> vÃ  <end_of_turn>) cho cáº£ user vÃ  model\n",
        "# Prompt Construction: Builds má»™t conversation context hoÃ n chá»‰nh (get_full_prompt) báº±ng cÃ¡ch káº¿t há»£p cáº£ history vÃ  system instructions\n",
        "# Gá»­i message: ThÃªm user input vÃ o history -> build prompt -> generate ra response báº±ng generate method cá»§a model -> update history vá»›i pháº£n há»“i tá»« model\n",
        "class ChatState():\n",
        "\n",
        "  __START_TURN_USER__ = \"<start_of_turn>user\\n\"\n",
        "  __START_TURN_MODEL__ = \"<start_of_turn>model\\n\"\n",
        "  __END_TURN__ = \"<end_of_turn>\\n\"\n",
        "\n",
        "  def __init__(self, model, system=\"\"):\n",
        "\n",
        "    self.model = model\n",
        "    self.system = system\n",
        "    self.history = []\n",
        "\n",
        "  def add_to_history_as_user(self, message):\n",
        "\n",
        "      self.history.append(self.__START_TURN_USER__ + message + self.__END_TURN__)\n",
        "\n",
        "  def add_to_history_as_model(self, message):\n",
        "\n",
        "      self.history.append(self.__START_TURN_MODEL__ + message)\n",
        "\n",
        "  def get_history(self):\n",
        "\n",
        "      return \"\".join([*self.history])\n",
        "\n",
        "  def get_full_prompt(self):\n",
        "\n",
        "    prompt = self.get_history() + self.__START_TURN_MODEL__\n",
        "    if len(self.system)>0:\n",
        "      prompt = self.system + \"\\n\" + prompt\n",
        "    return prompt\n",
        "\n",
        "  def send_message(self, message):\n",
        "\n",
        "    self.add_to_history_as_user(message)\n",
        "    prompt = self.get_full_prompt()\n",
        "    response = self.model.generate(prompt, max_length=2048)\n",
        "    result = response.replace(prompt, \"\")\n",
        "    self.add_to_history_as_model(result)\n",
        "    return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9hmJS4h4ZmiP"
      },
      "source": [
        "### Chat vá»›i model\n",
        "\n",
        "Báº¯t Ä‘áº§u chat vá»›i model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1913181",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "outputId": "e048bcae-0753-4505-ce3d-9534153f63b7"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>ğŸ™‹â€â™‚ï¸<blockquote>Explain Riemann hypothesis to me like I'm a high schoooler</blockquote></font><font size='+1' color='teal'>ğŸ¤–\n\n> Imagine you have a giant, endless sheet of paper.  You want to fill it with numbers, but there's a catch: you can only use whole numbers.  \n> \n> Now, imagine you're trying to find all the \"prime numbers\" on this sheet. Prime numbers are like the special, unique building blocks of numbers. They can only be divided by 1 and themselves.  \n> \n> The Riemann Hypothesis is about understanding the distribution of these prime numbers on this paper. It's a big, complex mathematical puzzle that scientists have been trying to solve for over a century.\n> \n> Here's the gist:\n> \n> * **Prime Numbers:**  Prime numbers are like the special, unique building blocks of numbers. They can only be divided by 1 and themselves.\n> * **The Riemann Zeta Function:** This is a special mathematical function that helps us understand the distribution of prime numbers. It's like a map that shows us where prime numbers are likely to appear.\n> * **The Riemann Hypothesis:** This hypothesis states that the distribution of prime numbers follows a very specific pattern, and this pattern can be described by the Riemann Zeta Function.\n> \n> **Why is it so important?**\n> \n> * **Cryptography:** Prime numbers are the foundation of modern encryption, which keeps our online information safe. Understanding the Riemann Hypothesis could help us develop even stronger encryption methods.\n> * **Number Theory:** The Riemann Hypothesis is a fundamental problem in number theory, the study of numbers and their properties. Solving it would revolutionize our understanding of numbers.\n> * **Mathematics in general:**  It's a big deal because it connects different areas of mathematics, like algebra, geometry, and analysis.\n> \n> **The Challenge:**\n> \n> The Riemann Hypothesis is incredibly difficult to prove. It's like trying to solve a puzzle with millions of pieces, and even the most brilliant mathematicians have struggled to crack it.\n> \n> **In a nutshell:**\n> \n> The Riemann Hypothesis is a big, unsolved puzzle about prime numbers and their distribution. It's a fascinating problem that could have major implications for cryptography, mathematics, and our understanding of the universe. \n> \n> \n> Let me know if you'd like me to explain any of these concepts in more detail! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 22
        }
      ],
      "source": [
        "# Khá»Ÿi táº¡o chatbot vÃ  gá»­i query Ä‘áº¿n chatbot\n",
        "chat = ChatState(gemma_lm)\n",
        "message = \"Explain Riemann hypothesis to me like I'm a high schoooler\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7448005b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "96eb2e43-0ae4-4a66-bc26-86e16a9a6cda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>ğŸ™‹â€â™‚ï¸<blockquote>Gimme code to compute all prime numbers up to 100</blockquote></font><font size='+1' color='teal'>ğŸ¤–\n\n> ```python\n> def is_prime(num):\n>   \"\"\"Checks if a number is prime.\"\"\"\n>   if num <= 1:\n>     return False\n>   if num <= 3:\n>     return True\n>   if num % 2 == 0 or num % 3 == 0:\n>     return False\n>   i = 5\n>   while i * i <= num:\n>     if num % i == 0 or num % (i + 2) == 0:\n>       return False\n>     i += 6\n>   return True\n> \n> primes = []\n> for num in range(2, 101):\n>   if is_prime(num):\n>     primes.append(num)\n> \n> print(\"Prime numbers up to 100:\", primes)\n> ```\n> \n> **Explanation:**\n> \n> 1. **`is_prime(num)` function:**\n>    - Takes an integer `num` as input.\n>    - Returns `True` if `num` is prime, `False` otherwise.\n>    - Optimizations:\n>      - Handles numbers less than or equal to 3 directly.\n>      - Checks divisibility by 2 and 3 first.\n>      - Uses the 6k Â± 1 optimization: All prime numbers greater than 3 can be expressed in the form 6k Â± 1 (where k is any integer).\n> \n> 2. **Main loop:**\n>    - Iterates through numbers from 2 to 100.\n>    - For each number, calls the `is_prime()` function.\n>    - If the function returns `True`, the number is added to the `primes` list.\n> \n> 3. **Output:**\n>    - Prints the list of prime numbers found.\n> \n> **How it works:**\n> \n> The code efficiently checks for primality by:\n> \n> - **Early optimization:**  It handles special cases for numbers less than or equal to 3.\n> - **6k Â± 1 optimization:**  It checks divisibility only by numbers of the form 6k Â± 1, significantly reducing the number of checks.\n> \n> \n> Let me know if you'd like to explore other prime number algorithms or have any more questions! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "message = \"Gimme code to compute all prime numbers up to 100\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0c51f42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 730
        },
        "outputId": "fe2d0cbf-9730-4deb-ff2e-0ab74baa25f1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>ğŸ™‹â€â™‚ï¸<blockquote>Great! Now add those explanations as comments in the code.</blockquote></font><font size='+1' color='teal'>ğŸ¤–\n\n> ```python\n> def is_prime(num):\n>   \"\"\"Checks if a number is prime.\n> \n>   Args:\n>     num: The number to check.\n> \n>   Returns:\n>     True if num is prime, False otherwise.\n>   \"\"\"\n>   if num <= 1:  # Numbers less than or equal to 1 are not prime\n>     return False\n>   if num <= 3:  # 2 and 3 are prime\n>     return True\n>   if num % 2 == 0 or num % 3 == 0:  # Check divisibility by 2 and 3\n>     return False\n>   i = 5  # Start checking divisibility from 5\n>   while i * i <= num:  # Check divisibility up to the square root of num\n>     if num % i == 0 or num % (i + 2) == 0:  # If divisible by i or i+2, not prime\n>       return False\n>     i += 6  # Increment by 6 to check the next potential divisors\n>   return True  # If no divisors found, it's prime\n> \n> primes = []  # Initialize an empty list to store prime numbers\n> for num in range(2, 101):  # Iterate through numbers from 2 to 100\n>   if is_prime(num):  # Check if the number is prime\n>     primes.append(num)  # If prime, add it to the list\n> \n> print(\"Prime numbers up to 100:\", primes)  # Print the list of prime numbers\n> ``` \n> \n> \n> Let me know if you have any other questions! \n> <end_of_turn>\n</font>"
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "message = \"Great! Now add those explanations as comments in the code.\"\n",
        "display_chat(message, chat.send_message(message))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51a33627"
      },
      "source": [
        "Test thá»­ code mÃ  model Ä‘Æ°a luÃ´n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "221c0817",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9895d7ff-9629-4687-bb09-bcbef0f45e4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prime numbers up to 100: [2, 3, 5, 7, 11, 13, 17, 19, 23, 29, 31, 37, 41, 43, 47, 53, 59, 61, 67, 71, 73, 79, 83, 89, 97]\n"
          ]
        }
      ],
      "source": [
        "def is_prime(num):\n",
        "  \"\"\"Checks if a number is prime.\n",
        "\n",
        "  Args:\n",
        "    num: The number to check.\n",
        "\n",
        "  Returns:\n",
        "    True if num is prime, False otherwise.\n",
        "  \"\"\"\n",
        "  if num <= 1:  # Numbers less than or equal to 1 are not prime\n",
        "    return False\n",
        "  if num <= 3:  # 2 and 3 are prime\n",
        "    return True\n",
        "  if num % 2 == 0 or num % 3 == 0:  # Check divisibility by 2 and 3\n",
        "    return False\n",
        "  i = 5  # Start checking divisibility from 5\n",
        "  while i * i <= num:  # Check divisibility up to the square root of num\n",
        "    if num % i == 0 or num % (i + 2) == 0:  # If divisible by i or i+2, not prime\n",
        "      return False\n",
        "    i += 6  # Increment by 6 to check the next potential divisors\n",
        "  return True  # If no divisors found, it's prime\n",
        "\n",
        "primes = []  # Initialize an empty list to store prime numbers\n",
        "for num in range(2, 101):  # Iterate through numbers from 2 to 100\n",
        "  if is_prime(num):  # Check if the number is prime\n",
        "    primes.append(num)  # If prime, add it to the list\n",
        "\n",
        "print(\"Prime numbers up to 100:\", primes)  # Print the list of prime numbers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from IPython.display import display  # Ensures Markdown rendering\n",
        "\n",
        "def timed_chat(message):\n",
        "    start_time = time.time()\n",
        "    response = chat.send_message(message)\n",
        "    total_time = time.time() - start_time\n",
        "    tokens_generated = len(response.split())  # Approximate token count\n",
        "    tokens_per_sec = tokens_generated / total_time if total_time > 0 else 0\n",
        "\n",
        "    # âœ… Ensure chat response is displayed properly\n",
        "    display(display_chat(message, response))\n",
        "\n",
        "    # âœ… Print generation stats\n",
        "    print(f\"\\nâ±ï¸ Total Generation Time: {total_time:.3f} sec | âš¡ Tokens/sec: {tokens_per_sec:.2f}\")\n"
      ],
      "metadata": {
        "id": "Sk3vkYm2VC_0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "timed_chat(\"Explain Riemann hypothesis to me like I'm a high schooler\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 872
        },
        "id": "IxOZ214SVFo5",
        "outputId": "2b07b6ad-8c29-4b48-e775-b383bc08c990"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "<font size='+1' color='brown'>ğŸ™‹â€â™‚ï¸<blockquote>Explain Riemann hypothesis to me like I'm a high schooler</blockquote></font><font size='+1' color='teal'>ğŸ¤–\n\n> Okay, imagine a giant, endless sheet of paper.  You want to fill it with numbers, but there's a catch: you can only use whole numbers.  \n> \n> Now, imagine you're trying to find all the \"prime numbers\" on this sheet. Prime numbers are like the special, unique building blocks of numbers. They can only be divided by 1 and themselves. Think of 2, 3, 5, 7, 11... those are prime numbers.\n> \n> The Riemann Hypothesis is about understanding how these prime numbers are spread out on this paper. It's a big, complex mathematical puzzle that scientists have been trying to solve for over a century.\n> \n> Here's the gist:\n> \n> * **Prime Numbers:** Prime numbers are like the special, unique building blocks of numbers. They can only be divided by 1 and themselves.\n> * **The Riemann Zeta Function:** This is a special mathematical function that helps us understand the distribution of prime numbers. It's like a map that shows us where prime numbers are likely to appear.\n> * **The Riemann Hypothesis:** This hypothesis states that the distribution of prime numbers follows a very specific pattern, and this pattern can be described by the Riemann Zeta Function.\n> \n> **Why is it so important?**\n> \n> * **Cryptography:** Prime numbers are the foundation of modern encryption, which keeps our online information safe. Understanding the Riemann Hypothesis could help us develop even stronger encryption methods.\n> * **Number Theory:** The Riemann Hypothesis is a fundamental problem in number theory, the study of numbers and their properties. Solving it would revolutionize our understanding of numbers.\n> * **Mathematics in general:**  It's a big deal because it connects different areas of mathematics, like algebra, geometry, and analysis.\n> \n> **The Challenge:**\n> \n> The Riemann Hypothesis is incredibly difficult to prove. It's like trying to solve a puzzle with millions of pieces, and even the most brilliant mathematicians have struggled to crack it.\n> \n> **In a nutshell:**\n> \n> The Riemann Hypothesis is a big, unsolved puzzle about prime numbers and their distribution. It's a fascinating problem that could have major implications for cryptography, mathematics, and our understanding of the universe. \n> \n> \n> Let me know if you'd like me to explain any of these concepts in more detail! \n> <end_of_turn>\n</font>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "â±ï¸ Total Generation Time: 29.055 sec | âš¡ Tokens/sec: 12.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c8ece6c"
      },
      "source": [
        "XÃ i `get_history` method Ä‘á»ƒ xem context Ä‘Æ°á»£c retained bá»Ÿi `Chat` class nhÆ° nÃ o"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e48f4ca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "591aa747-47f3-4871-a1c1-a71363708d42"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<start_of_turn>user\n",
            "Tell me, in a few words,  how to compute all prime numbers up to 1000?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "**Sieve of Eratosthenes.** \n",
            "<end_of_turn><start_of_turn>user\n",
            "Now in Python! No numpy, please!<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Returns a list of prime numbers up to n.\"\"\"\n",
            "  primes = [True] * (n + 1)\n",
            "  primes[0] = primes[1] = False\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    if primes[i]:\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        primes[j] = False\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1. **Initialization:**\n",
            "   - `primes = [True] * (n + 1)`: Creates a list `primes` of boolean values, initially assuming all numbers are prime.\n",
            "   - `primes[0] = primes[1] = False`: Sets 0 and 1 as non-prime.\n",
            "\n",
            "2. **Iteration:**\n",
            "   - `for i in range(2, int(n**0.5) + 1):`: Iterates from 2 to the square root of `n`. We only need to check up to the square root because any composite number must have a prime factor less than or equal to its square root.\n",
            "   - `if primes[i]:`: If `i` is marked as prime:\n",
            "     - `for j in range(i * i, n + 1, i):`: Marks all multiples of `i` as non-prime.\n",
            "\n",
            "3. **Result:**\n",
            "   - `return [i for i, is_prime in enumerate(primes) if is_prime]`: Creates a list of indices where `primes[i]` is True, representing the prime numbers.\n",
            "\n",
            "\n",
            "Let me know if you'd like a more detailed explanation of any part! \n",
            "<end_of_turn><start_of_turn>user\n",
            "Thank you, it works! Can you explain the code in French?<end_of_turn>\n",
            "<start_of_turn>model\n",
            "Bien sÃ»r ! Voici une explication du code en franÃ§ais :\n",
            "\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Retourne une liste de nombres premiers jusqu'Ã  n.\"\"\"\n",
            "  primes = [True] * (n + 1)\n",
            "  primes[0] = primes[1] = False\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    if primes[i]:\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        primes[j] = False\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explication:**\n",
            "\n",
            "1. **Initialisation:**\n",
            "   - `primes = [True] * (n + 1)`: CrÃ©e une liste `primes` de valeurs boolÃ©ennes, initialement supposant que tous les nombres sont premiers.\n",
            "   - `primes[0] = primes[1] = False`: DÃ©finit 0 et 1 comme non-premiers.\n",
            "\n",
            "2. **ItÃ©ration:**\n",
            "   - `for i in range(2, int(n**0.5) + 1):`: ItÃ¨re de 2 jusqu'Ã  la racine carrÃ©e de `n`. Nous ne devons vÃ©rifier que jusqu'Ã  la racine carrÃ©e car tout nombre composite doit avoir un facteur premier infÃ©rieur ou Ã©gal Ã  sa racine carrÃ©e.\n",
            "   - `if primes[i]:`: Si `i` est considÃ©rÃ© comme premier:\n",
            "     - `for j in range(i * i, n + 1, i):`:  Marquer tous les multiples de `i` comme non-premiers.\n",
            "\n",
            "3. **Resultat:**\n",
            "   - `return [i for i, is_prime in enumerate(primes) if is_prime]`: CrÃ©e une liste des indices oÃ¹ `primes[i]` est vrai, reprÃ©sentant les nombres premiers.\n",
            "\n",
            "\n",
            "N'hÃ©sitez pas Ã  me demander si vous avez besoin d'une explication plus dÃ©taillÃ©e de quelque chose! \n",
            "<end_of_turn><start_of_turn>user\n",
            "Great! Now add those explanations as comments in the code.<end_of_turn>\n",
            "<start_of_turn>model\n",
            "```python\n",
            "def sieve_of_eratosthenes(n):\n",
            "  \"\"\"Retourne une liste de nombres premiers jusqu'Ã  n.\"\"\"\n",
            "  # Initialise une liste de boolean avec True pour tous les nombres de 0 Ã  n\n",
            "  primes = [True] * (n + 1)\n",
            "  # DÃ©finit 0 et 1 comme non-premiers\n",
            "  primes[0] = primes[1] = False\n",
            "  # ItÃ¨re de 2 Ã  la racine carrÃ©e de n\n",
            "  for i in range(2, int(n**0.5) + 1):\n",
            "    # Si i est considÃ©rÃ© comme premier\n",
            "    if primes[i]:\n",
            "      # ItÃ¨re sur tous les multiples de i\n",
            "      for j in range(i * i, n + 1, i):\n",
            "        # DÃ©finit les multiples de i comme non-premiers\n",
            "        primes[j] = False\n",
            "  # Retourne la liste des indices des nombres premiers\n",
            "  return [i for i, is_prime in enumerate(primes) if is_prime]\n",
            "\n",
            "primes = sieve_of_eratosthenes(1000)\n",
            "print(primes)\n",
            "```\n",
            "\n",
            "**Explication:**\n",
            "\n",
            "* **Initialisation:**\n",
            "    * `primes = [True] * (n + 1)`:  CrÃ©e une liste `primes` de valeurs boolÃ©ennes, initialement supposant que tous les nombres sont premiers.\n",
            "    * `primes[0] = primes[1] = False`: DÃ©finit 0 et 1 comme non-premiers.\n",
            "* **ItÃ©ration:**\n",
            "    * `for i in range(2, int(n**0.5) + 1):`: ItÃ¨re de 2 jusqu'Ã  la racine carrÃ©e de `n`. Nous ne devons vÃ©rifier que jusqu'Ã  la racine carrÃ©e car tout nombre composite doit avoir un facteur premier infÃ©rieur ou Ã©gal Ã  sa racine carrÃ©e.\n",
            "    * `if primes[i]:`: Si `i` est considÃ©rÃ© comme premier:\n",
            "        * `for j in range(i * i, n + 1, i):`:  Marquer tous les multiples de `i` comme non-premiers.\n",
            "* **Resultat:**\n",
            "    * `return [i for i, is_prime in enumerate(primes) if is_prime]`: CrÃ©e une liste des indices oÃ¹ `primes[i]` est vrai, reprÃ©sentant les nombres premiers. \n",
            "\n",
            "\n",
            "\n",
            "<end_of_turn>\n"
          ]
        }
      ],
      "source": [
        "print(chat.get_history())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9693c66f"
      },
      "source": [
        "## Tá»•ng káº¿t vÃ  Ä‘á»c thÃªm\n",
        "\n",
        "Váº­y lÃ  xong pháº§n build chatbot vá»›i Gemma 2B Instruction tuned model xÃ i Keras on JAX.\n",
        "\n",
        "Äá»c thÃªm:\n",
        "\n",
        "* [Get started with Keras Gemma](https://ai.google.dev/gemma/docs/get_started).\n",
        "* [Finetune the Gemma model on GPU](https://ai.google.dev/gemma/docs/lora_tuning).\n",
        "* [Gemma integration with Vertex AI](https://ai.google.dev/gemma/docs/integrations/vertex)\n",
        "* [Gemma on Vertex AI](https://cloud.google.com/vertex-ai/docs/generative-ai/open-models/use-gemma).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Æ¯u Ä‘iá»ƒm cá»§a setup sá»­ dá»¥ng Google Colab\n",
        "* *Ease of setup*: Code vÃ  dependencies minimal hÆ¡n, trong khi Local thÃ¬ cáº§n nhiá»u setup hÆ¡n cho tokenizer, model vÃ  server. keras_nlp.models.GemmaCausalLM.from_preset nÃ³ load trá»±c tiáº¿p nguyÃªn má»™t cÃ¡i pre-trained model Ä‘Ã£ Ä‘Æ°á»£c tá»‘i Æ°u, khÃ´ng cáº§n pháº£i lo thÃªm vá» handling, memory optimization hay set up tokenizer\n",
        "* *Harware Uitilization*: TÆ°Æ¡ng thÃ­ch vá»›i T4 GPU(qua Jax), Local thÃ¬ cáº§n cáº¥u hÃ¬nh thá»§ cÃ´ng cho CPU vÃ  GPU\n",
        "* *Model loading*: Pre-trained thÃ´ng qua preset cá»§a Keras-NLP, Local thÃ¬ cáº§n loading thá»§ cÃ´ng thÃ´ng qua transformers, cáº§n nhiá»u VRAM hÆ¡n\n",
        "* *Framework*: Jax(nhanh hÆ¡n vÃ  tá»‘i Æ°u trÃªn TPU GPU chuyÃªn dá»¥ng)\n",
        "* *UI*: Colab(Markdown trong Colab, module IPython cá»§a Jupyter gá»n nháº¹)\n",
        "* *Tá»‘c Ä‘á»™ Inference*: Nhá» Jax nÃªn ráº¥t tá»‘i Æ°u lÃ  má»™t, Keras-NLP láº¡i cÃ²n lo háº¿t tá»« batching tá»›i memory tá»›i token generation ná»¯a nÃªn siÃªu tá»‘i Æ°u, cÃ²n giáº£i phÃ¡p Local thÃ¬ cáº§n tá»± lÃ m batching, cÅ©ng kha khÃ¡ phá»©c táº¡p vÃ  tÄƒng batch size lÃªn thÃ¬ cÅ©ng cáº§n nhiá»u vram, vá»›i mÃ¬nh thá»­ tá»± set batch_size thÃ¬ lá»—i nháº£y Ä‘á» lÃ¨ nÃªn hong lÃ m:>\n",
        "* *Backend*: Colab khÃ´ng cÃ³ server overhead luÃ´n mÃ  lÃ  direct execution, má»i thá»© nÃ³ cháº¡y trong háº¿t cÃ¹ng 1 process."
      ],
      "metadata": {
        "id": "pDYv8pETF-qh"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}